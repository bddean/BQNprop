# Utilities and hyperparameters.
learning_rate â† 0.5
MxV â† +Ëâˆ˜Ã—â‰1â€¿âˆ # Matrix Ã— Vector multiplication via BQNCrate
Sigmoid â† {Ã·1+â‹†-ğ•©}
random â† â€¢MakeRand 18210

# Initialize weights for ğ•©, a list containing each layer's size.
Create â‡ {CreateLayerÂ¨<Ë˜2â†•ğ•©}
CreateLayer â† {Â¯1 + 0.01 Ã— ğ•©â¥Š(Ã—Â´ğ•©)random.Range 200}

# Run a trained network; pass input ğ•¨ through weights ğ•©.
Compute â‡ Sigmoidâˆ˜MxVËœÂ´âŸœâŒ½

# BackProp performs the backpropagation algorithm to return an incrementally
# improved set of weights ğ•©.
# 
# Glossary:
#   z :: weighted node output at each row
#   a :: activation values (`Sigmoid z`)
#   e :: activation error (Î´ in http://neuralnetworksanddeeplearning.com/chap2.htmlreference)
#   ğ•© :: model (weights matrix-list)
#   dğ•¨dğ•© :: derivative of some value ğ•¨ w.r.t some ğ•©
BackProp â‡ {
	âŸ¨input, targetâŸ© â† ğ•¨
	aâ†(<input) Sigmoidâˆ˜MxV` ğ•©
	# Conveniently, sigmoid's derivative `dzda` can be expressed in terms of
	# its output `a`.
	dzda â† Ã—âŸœÂ¬ a
	eLâ†(Â¯1âŠ‘dzda) Ã— 2 Ã— (Â¯1âŠ‘a) - target
	# Now *propagate* error from (o)utput *back* to (i)nput.
	e â† { eo â† ğ•¨ â‹„ woâ€¿d â† ğ•©
		d Ã— wo MxV eo
	}`âŒ¾âŒ½ âŸ¨eLâŸ© âˆ¾Ëœ <Ë˜ 1â†“ğ•©â‰Ë˜Â»dzda
	ğ•© - learning_rate Ã— (âŸ¨inputâŸ©Â»a) Ã—âŒœÂ¨ e
}

Test â‡ {â‹„â‹„â‹„ ğ•¤ â‹„â‹„â‹„
	iâ†0.7â€¿0.8â€¿0.9
	mâ†Create 3â€¿4â€¿4â€¿4â€¿4â€¿4â€¿5â€¿4â€¿3
	expâ†1â€¿0.5â€¿0.1
	â€¢Show "		~~ EXPECT:"
	â€¢Show exp
	â€¢Show "		~~ COMPUTE - first:"
	â€¢Show i Compute m
	mâ†© { input â‡i, target â‡ exp } BackPropâŸ75 m
	â€¢Show "		~~ COMPUTE - trained:"
	â€¢Show i Compute m
}
Test @
